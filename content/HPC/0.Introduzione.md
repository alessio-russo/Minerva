Nell'ambito dell'High Performance Computing (HPC) si affrontano sfide complesse che abbracciano vari settori. Questi problemi hanno un impatto significativo sul progresso scientifico e tecnologico e, sebbene possano essere affrontati attraverso soluzioni computazionali, richiedono capacità di calcolo e spazi di archiviazione eccezionalmente elevati per essere risolti in tempi accettabili.

## La programmazione parallela

La legge di Moore, formulata nel 1965, prevedeva un raddoppio delle capacità dei microprocessori ogni 18 mesi. Tuttavia, intorno al 2005, si sono iniziati a percepire i limiti di questa previsione a causa delle restrizioni fisiche legate alla miniaturizzazione dei transistor. Di conseguenza, la frequenza operativa dei processori si è assestata in un intervallo tra 2 e 3 GHz. Questo stallo ha spinto verso l'adozione di architetture multicore, che permettono di mantenere una crescita esponenziale delle prestazioni, distribuendole su più unità di elaborazione all'interno di un singolo chip. Per massimizzare l'efficienza di queste tecnologie avanzate, è fondamentale parallelizzare il carico di lavoro delle applicazioni, distribuendolo efficacemente tra i diversi core o sfruttando altre innovazioni hardware.

L'ottimizzazione e la parallelizzazione del flusso di esecuzione degli algoritmi possono essere implementate a vari livelli hardware:

- **All'interno di un singolo core**, tecniche come il pipelining e l'hyperthreading consentono di aumentare l'efficienza elaborativa sfruttando al meglio le capacità dell'unità di elaborazione. 
- **A livello di nodo**, l'adozione di architetture multicore e multi-socket permette di distribuire il carico di lavoro su più unità di elaborazione parallele. Inoltre, l'utilizzo di acceleratori hardware, come le GPU (Graphics Processing Units), fornisce un ulteriore livello di ottimizzazione per compiti computazionalmente intensivi. 
- **All'interno di un cluster**, la cooperazione tra più nodi offre la possibilità di affrontare problemi di grande scala, distribuendo il processo computazionale su un'ampia rete di risorse.

##### Architetture parallele

Le prestazioni superiori dei moderni sistemi HPC (High-Performance Computing) derivano dall'impiego di architetture hardware avanzate, che integrano diversi livelli di parallelismo. Questo approccio consente di combinare le capacità di elaborazione disponibili a vari livelli per massimizzare l'efficienza complessiva. 

Per sfruttare appieno queste architetture, è necessario scomporre il carico di lavoro in task paralleli, ciascuno dei quali è ottimizzato per specifici livelli di parallelismo mediante l'adozione di modelli di programmazione ad hoc.

I modelli principali includono:

- **Programmazione Multithreading**: Questo modello è ideale per il parallelismo multicore in architetture a memoria condivisa, permettendo a più thread di esecuzione di operare contemporaneamente su diversi core utilizzando una memoria comune. 

- **Programmazione a Scambio di Messaggi**: Applicata principalmente in architetture multinodo con memoria distribuita, questa modalità prevede la comunicazione tra processi distinti (tipicamente su nodi differenti) tramite lo scambio di messaggi. 

- **Programmazione di Kernel su Acceleratori (GPU)**: Gli acceleratori, come le GPU, offrono capacità di calcolo parallelo massivo per specifici tipi di carichi di lavoro, come l'elaborazione di dati vettoriali o matriciali. 


**Progettazione di programmi paralleli**

Dal punto di vista della programmazione, esistono diverse metodologie di progettazione per scomporre il carico computazionale nei task che verranno poi distribuiti sulle diverse unità di elaborazione. Questo processo di scomposizione deve tenere in considerazione diversi aspetti, tra cui l'**organizzazione dei dati**, le **componenti funzionali dell'algoritmo** e i **livelli di parallelismo offerti dal sistema di calcolo**. Un'attenzione particolare deve essere data alla maniera in cui i dati sono accessibili ai processori e alla modalità con cui le diverse parti dell'algoritmo possono essere eseguite in parallelo, minimizzando le dipendenze e l'overhead di comunicazione.

Lo **speedup** rappresenta il rapporto tra il tempo di esecuzione di un programma eseguito su un singolo processore (**Tseriale**) e il tempo di esecuzione dello stesso programma su un sistema parallelo (**Tparallelo**) : $\text{speedup} = \text{Tseriale} /Tparallelo$

In un contesto ideale, il valore di speedup è direttamente proporzionale al numero di processori utilizzati, ma nella pratica, questo ideale è spesso limitato da vari fattori, inclusi overhead di comunicazione e sincronizzazione.

La **scalabilità** descrive la capacità di un programma parallelo di aumentare il proprio speedup proporzionalmente all'aggiunta di ulteriori unità di processamento. La scalabilità è influenzata da diversi fattori, tra cui l'efficienza della decomposizione dei task, la gestione dell'overhead di parallelizzazione e la capacità del sistema di minimizzare i tempi morti dovuti alla comunicazione o alla sincronizzazione tra task.
##### Programmazione a memoria condivisa (Multithreading)

Le architetture a memoria condivisa sono spesso realizzata attraverso **sistemi SMP** (Symmetric Multi-Processor). In queste architetture, un insieme di processori omogenei opera in modo indipendente, ma ha accesso a uno spazio di indirizzamento di memoria RAM condiviso. Questo approccio permette ai task di un programma di essere eseguiti contemporaneamente su diverse unità di elaborazione, con la possibilità di comunicare e sincronizzarsi attraverso l'accesso a variabili condivise nella memoria.

La chiave per lo sviluppo efficiente di applicazioni in questo contesto è l'utilizzo di librerie software che facilitano la gestione dei thread e la sincronizzazione. Tra le più note e utilizzate troviamo:

- **Posix Threads (Pthreads)**: Una libreria standardizzata per il C che offre un set di API per la creazione e la gestione di thread, oltre alla sincronizzazione tra essi mediante mutex, semafori e variabili condizionali.

- **OpenMP (Open Multi-Processing)**: Una libreria e un insieme di direttive del compilatore progettate specificamente per la programmazione parallela a memoria condivisa. OpenMP supporta vari linguaggi di programmazione, tra cui C, C++, e si distingue per la sua facilità di utilizzo.

##### Programmazione a memoria distribuita

La programmazione a memoria distribuita si basa sull'utilizzo di un'architettura a memoria distribuita, la quale è configurata come un cluster di computer connessi tra loro tramite una rete di comunicazione avanzata. All'interno di questo cluster, ciascun nodo opera in maniera autonoma, eseguendo task o processi su propri processori, ognuno con un proprio spazio di indirizzamento privato. 

La sfida principale in un sistema di memoria distribuita risiede nella necessità di una comunicazione efficiente tra i vari task distribuiti. Per affrontare tale sfida, è essenziale l'impiego di specifiche librerie software progettate per facilitare lo scambio di messaggi e dati. Queste librerie permettono di implementare diversi modelli di comunicazione, tra cui la comunicazione punto-punto, che avviene direttamente tra coppie di task, e la comunicazione globale, che coinvolge simultaneamente tutti i task del cluster.

Uno degli standard più riconosciuti e ampiamente adottati per la programmazione di questo tipo di architettura è l'**MPI** (Message Passing Interface). Lo standard MPI fornisce una specifica dettagliata per lo sviluppo di librerie software che facilitano lo scambio di messaggi in modo efficiente e scalabile. Le implementazioni di MPI variano dall'open source, come Open MPI, che offre flessibilità e accessibilità, a soluzioni commerciali, quali Intel MPI.

##### Programmazione GPU

La programmazione di unità di elaborazione grafica (GPU) ha subito una notevole evoluzione dalla sua origine, trasformandosi da un semplice coprocessore dedicato al rendering di immagini per dispositivi di visualizzazione a un potente strumento di calcolo generico, segnando così la nascita delle GP-GPU (General Purpose GPU).

NVIDIA, uno dei principali produttori di GP-GPU, ha sviluppato il framework **CUDA** (Compute Unified Device Architecture), le librerie dedicate e il compilatore **nvcc**, consentendo la scrittura diretta di codice per le GPU NVIDIA. 

La programmazione CUDA consiste nel definire un kernel di calcolo, un piccolo programma che viene eseguito in parallelo sulla GPU. Questi kernel sono sviluppati dall'utente e inviati, insieme ai dati necessari, dall'unità di elaborazione centrale (CPU) all'host alla GPU. Una volta ricevuti, la GPU elabora i dati in parallelo, sfruttando la sua architettura ottimizzata per il calcolo parallelo, e al termine dell'elaborazione invia i risultati elaborati indietro alla CPU.

![culone bello](/content/HPC/_IMG/roofline-analysis.png)